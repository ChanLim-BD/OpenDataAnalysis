데이터 수집 vs 크롤링

웹사이트의 내용을 읽어오는 것 => 웹 데이터 수집, 스크래핑

검색엔진이 하이퍼링크를 타고 웹 페이지의 내용을 읽어가는 것 => 크롤링

---

* 로봇 배제 표준

- 로봇 배제 표준, 로봇 배제 프로토콜은 웹 사이트에 로봇이 접근하는 것을 방지하기 위한 규약으로, 일반적으로 접근제한에 대한 설명을 robot.txt에 기술.
- 이 규약은 권고안. 그러나 저작권 침해 주의

https://www.seoul.go.kr/robots.txt

* 저작권 확인

* 무리한 네트워크 요청 확인
- DDOS 공격으로 의심 받을 수 있음.
- time.sleep()으로 간격 두기

---

데이터 수집 방법

1. 수집 하고자 하는 페이지의 URL을 알아본다.
2. 파이썬의 작은 브라우저 requests를 통해 URL에 접근한다.
3. response.status_code 가 200 OK 라면 정상 응답입니다.
4. request의 response 값에서 response.text만 받아옵니다.
5. html 텍스트를 받아왔다면 bs(response.text, 'html.parser')로 html 을 해석합니다.
6. soup.select 를 통해 원하는 태그에 접근합니다.
7. 목록을 먼저 받아옵니다.
8. 목록에서 행을 하나씩 가져옵니다.
9. 행을 모아서 데이터프레임으로 만듭니다.

	=> 판다스라면 read_html로 table로 된 태그에 대해 위 과정을 코드 한 줄로 해줍니다.

---

JSON

- 속성-값 쌍 또는 키-값 쌍 으로 이루어진 데이터 오브젝트를 전달하기 위해 인간이 읽을 수 있는 텍스트를 사용하는 개방형 표준 포맷이다.

---

서울시 코로나 발생현황 데이터 수집 순서

1. 페이지별 데이터 수집
2. 전체 페이지를 수집
3. pd.concat([전체데이터리스트]) 로 데이터를 하나로 병합
4. 데이터 전처리 (html 태그 제거)
5. to_csv로 전체 데이터 병합
6. pd.read_csv 로 데이터가 잘 저장되었는지 읽어와서 확인
7. 수집 끝! 분석 시작!


---


